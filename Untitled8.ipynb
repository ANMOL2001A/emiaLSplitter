{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN0SEVSg/tEUtwxNJyRhB/B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANMOL2001A/emiaLSplitter/blob/master/Untitled8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYDg1x2UEOfh",
        "outputId": "d9360c59-afe9-443d-f965-f5d6625b1df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "ACjewMXAEMxX",
        "outputId": "5230d964-176c-4a3a-dc80-f133f8eb5ca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nsentence = [\"hello\",\"my\",\"self\",\"anmol\"]\\nvocab =[\"hey\",\"good\",\"anmol\",\"self\",\"my\"]\\nbag_of_words = bag_of_words(sentence, vocab)\\nprint(f\"sentence => {sentence}\")\\nprint(f\"vocabulary => {vocab}\")\\n\\nprint(bag_of_words)\\n\\na=\"hello,my self anmol sharma?\"\\nprint(a)\\nb= tokenize(a)\\nprint(b)\\nc = stemming(\"Running\")\\nfor token in b:\\n    print(f\"{token} -> {stemming(token)}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('punkt') #punkt tokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def tokenize(sentence):\n",
        "    return nltk.word_tokenize(sentence)\n",
        "\n",
        "def stemming(word):\n",
        "    return stemmer.stem(word.lower())\n",
        "\n",
        "\n",
        "\n",
        "def bag_of_words(tokenized_sentence, vocab):\n",
        "    # stem each word\n",
        "    sentence_words = []\n",
        "    for word in tokenized_sentence:\n",
        "        sentence_words.append(stemming(word))\n",
        "\n",
        "    # initialize bag with 0 for each word\n",
        "    bag = np.zeros(len(vocab), dtype=np.float32)\n",
        "    for idx, w in enumerate(vocab):\n",
        "        if w in sentence_words:\n",
        "            bag[idx] = 1\n",
        "\n",
        "    return bag\n",
        "\n",
        "#examples of tokenizing,stemming,bag of words\n",
        "\"\"\"\n",
        "sentence = [\"hello\",\"my\",\"self\",\"anmol\"]\n",
        "vocab =[\"hey\",\"good\",\"anmol\",\"self\",\"my\"]\n",
        "bag_of_words = bag_of_words(sentence, vocab)\n",
        "print(f\"sentence => {sentence}\")\n",
        "print(f\"vocabulary => {vocab}\")\n",
        "\n",
        "print(bag_of_words)\n",
        "\n",
        "a=\"hello,my self anmol sharma?\"\n",
        "print(a)\n",
        "b= tokenize(a)\n",
        "print(b)\n",
        "c = stemming(\"Running\")\n",
        "for token in b:\n",
        "    print(f\"{token} -> {stemming(token)}\")\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.l1 = nn.Linear(input_size, hidden_size)\n",
        "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.l2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.l3(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "EvNymyQRhAmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "with open('/content/Mario_intentswise_dataset.json','r') as f:\n",
        "            my_file =json.load(f)\n",
        "\n",
        "\n",
        "vocab =[]\n",
        "tags =[]\n",
        "xy =[]#it will hold both the tags and pattern\n",
        "for intent in my_file['intents']:\n",
        "    tag= intent['tag']\n",
        "    tags.append(tag)\n",
        "    for pattern in intent['patterns']:\n",
        "      w = tokenize(pattern)\n",
        "      vocab.extend(w)#w is also an array and vacab is also an array so in append it will become arrays of array\n",
        "        # add to xy pair\n",
        "      xy.append((w, tag))\n",
        "\n",
        "# stem and lower each word\n",
        "ignore_words = ['?', '.', '!']\n",
        "print(vocab)\n",
        "new_vocab = []\n",
        "for w in vocab:\n",
        "    if w not in ignore_words:\n",
        "        new_vocab.append(stemming(w))\n",
        "\n",
        "vocab = new_vocab\n",
        "\n",
        "# # remove duplicates and sort\n",
        "vocab = sorted(set(vocab))\n",
        "print(vocab)\n",
        "tags = sorted(set(tags))\n",
        "\n",
        "\n",
        "X_train = []#hold associated numbers for each pattern and vocab\n",
        "y_train = []\n",
        "for (pattern_sentence, tag) in xy:\n",
        "    # X: bag of words for each pattern_sentence\n",
        "    bag = bag_of_words(pattern_sentence, vocab)\n",
        "    X_train.append(bag)\n",
        "    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot\n",
        "    label = tags.index(tag)#index numbers of each tags\n",
        "    y_train.append(label)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 1000\n",
        "batch_size = 8\n",
        "learning_rate = 0.001\n",
        "input_size = len(X_train[0])\n",
        "hidden_size = 8\n",
        "output_size = len(tags)\n",
        "print(input_size, output_size)\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_samples = len(X_train)\n",
        "        self.x_data = X_train\n",
        "        self.y_data = y_train\n",
        "\n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "dataset = ChatDataset()\n",
        "train_loader = DataLoader(dataset=dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    for (words, labels) in train_loader:\n",
        "        words = words.to(device)\n",
        "        labels = labels.to(dtype=torch.long).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(words)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "print(f'final loss: {loss.item():.4f}')\n",
        "\n",
        "data = {\n",
        "\"model_state\": model.state_dict(),\n",
        "\"input_size\": input_size,\n",
        "\"hidden_size\": hidden_size,\n",
        "\"output_size\": output_size,\n",
        "\"vocab\": vocab,\n",
        "\"tags\": tags\n",
        "}\n",
        "\n",
        "FILE = \"mario_data.pth\"\n",
        "torch.save(data, FILE)\n",
        "\n",
        "print(f'training complete. file saved to {FILE}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "DCN7jbOfGYkN",
        "outputId": "22b65dc2-e2e0-4106-c04e-33f96e5dcf67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6660c988f3bd>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Mario_intentswise_dataset.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mmy_file\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Mario_intentswise_dataset.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "with open('/content/Mario_intentswise_dataset.json', 'r') as json_data:\n",
        "    intents = json.load(json_data)\n",
        "\n",
        "FILE = \"/content/mario_data.pth\"\n",
        "data = torch.load(FILE)\n",
        "\n",
        "input_size = data[\"input_size\"]\n",
        "hidden_size = data[\"hidden_size\"]\n",
        "output_size = data[\"output_size\"]\n",
        "vocab = data['vocab']\n",
        "tags = data['tags']\n",
        "model_state = data[\"model_state\"]\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "model.load_state_dict(model_state)\n",
        "model.eval()\n",
        "\n",
        "bot_name = \"Mario_Game_Helper\"\n",
        "print(\"Let's chat! (type 'quit' to exit)\")\n",
        "while True:\n",
        "    # sentence = \"do you use credit cards?\"\n",
        "    sentence = input(\"You: \")\n",
        "    if sentence == \"quit\":\n",
        "        break\n",
        "\n",
        "    sentence = tokenize(sentence)\n",
        "    X = bag_of_words(sentence, vocab)\n",
        "    X = X.reshape(1, X.shape[0])\n",
        "    X = torch.from_numpy(X).to(device)\n",
        "\n",
        "    output = model(X)\n",
        "    _, predicted = torch.max(output, dim=1)\n",
        "\n",
        "    tag = tags[predicted.item()]\n",
        "\n",
        "    probs = torch.softmax(output, dim=1)\n",
        "    prob = probs[0][predicted.item()]\n",
        "    if prob.item() > 0.75:\n",
        "        for intent in intents['intents']:\n",
        "            if tag == intent[\"tag\"]:\n",
        "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
        "    else:\n",
        "        print(f\"{bot_name}: I do not understand...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9rK5gktCDup",
        "outputId": "f7816751-a67e-400d-e48f-a0771be58621"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's chat! (type 'quit' to exit)\n",
            "You: hello\n",
            "Mario_Game_Helper: Hello, thanks for visiting\n",
            "You: good day\n",
            "Mario_Game_Helper: Hi there, what can I do for you?\n",
            "You: who is mario\n",
            "Mario_Game_Helper: Mario is a fictional character and the iconic hero of the Mario franchise. He's a plumber from the Mushroom Kingdom and is known for his red cap and blue overalls. Mario's main quest is to rescue Princess Peach from the clutches of the villainous Bowser.\n",
            "You: huhij\n",
            "Mario_Game_Helper: Hello, thanks for visiting\n",
            "You: who is {mask}\n",
            "Mario_Game_Helper: Mario is a fictional character and the iconic hero of the Mario franchise. He's a plumber from the Mushroom Kingdom and is known for his red cap and blue overalls. Mario's main quest is to rescue Princess Peach from the clutches of the villainous Bowser.\n",
            "You: who is\n",
            "Mario_Game_Helper: Mario is a fictional character and the iconic hero of the Mario franchise. He's a plumber from the Mushroom Kingdom and is known for his red cap and blue overalls. Mario's main quest is to rescue Princess Peach from the clutches of the villainous Bowser.\n",
            "You: ggg\n",
            "Mario_Game_Helper: Hi there, how can I help?\n",
            "You: hii\n",
            "Mario_Game_Helper: Hey :-)\n",
            "You: gg\n",
            "Mario_Game_Helper: Hi there, what can I do for you?\n",
            "You: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QoMPZZHDlIof"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}